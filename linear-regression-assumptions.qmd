---
bibliography: references.bib
---

# Assumptions of linear regression {#sec-linear-regression-assumptions}

```{r}
#| include: false

library(ggplot2)
library(gt)

Plot_theme <- source("functions/plot_theme.R")
Plot_theme <- Plot_theme$value
```


Linear regression is one of the most powerful and versatile statistical tools available for research, prediction, and hypothesis evaluation. However, the [validity]{style="text-decoration: underline;"} and [reliability]{style="text-decoration: underline;"} of a linear regression model are heavily dependent on several [key assumptions]{style="color:green;"} that form the foundation of its mathematical framework. When these assumptions are met, the model's [coefficients]{style="text-decoration: underline;"}, [predictions]{style="text-decoration: underline;"}, and [statistical inferences]{style="text-decoration: underline;"} are [trustworthy]{style="text-decoration: underline;"}. If these assumptions are violated, however, the conclusions drawn from the model can become [biased]{style="color:red;"}, [inefficient]{style="color:red;"}, or outright [misleading]{style="color:green;"}.

The critical issue here is that your computer won’t warn you if the assumptions are violated. The model will happily compute coefficients and output results regardless of whether the assumptions hold. Your PC follows your instructions—it has no way of assessing the validity of the assumptions underpinning your analysis. This responsibility falls squarely on you, the scientist, as an expert in both the data and the subject matter. This is not just a technical formality or a check box to be ticked when submitting research papers, but a step towards ensuring that the insights and predictions derived from the model are both accurate and meaningful. Treat it as a professional responsibility.


::: callout-warning
Assessing whether your proposed modelling strategy [fulfils its assumptions]{style="text-decoration: underline;"} is one of your tasks as a scientist. Neither R, SPSS nor STATA will display a warning window if assumptions are not met.
:::

Please keep in mind that every statistical model is an [extreme simplification of reality]{style="text-decoration: underline;"}. Think of models as lenses through which we interpret complex relationships in data, using the best tools at our disposal to generate estimates and predictions. Importantly, violating a model’s assumptions doesn’t necessarily invalidate the model outright. However, it does raise critical questions about the model’s appropriateness for the data and the reliability of its conclusions.

In this section, we will explore five fundamental assumptions of linear regression: linearity, independence, homoscedasticity, normality (of residuals) and no multicollinearity.   


## Linearity

The assumption of linearity is perhaps the most foundational aspect of linear regression—it’s even in the name! This assumption states that the relationship between the predictor(s) and the response variable is linear, meaning that [changes in the predictor lead to proportional changes in the response]{style="color:green;"}. this relationship is described by @eq-linear-regression-simple.

If the assumption of linearity is [violated]{style="color:red;"}, the model may fail to capture the true nature and magnitude of the relationship, leading to biased estimates and poor predictive performance. As a neuroscientist, it is your responsibility—not the computer’s—to determine whether the relationship between your variables is likely to be linear. This judgment is informed by Exploratory Data Analysis (EDA), where you visualize and assess the nature of the data relationships. Your expertise and understanding of the biological context play a pivotal role in this step.

The importance of linearity lies in the goal of linear regression: to estimate the [best-fit line]{style="color:green;"}, —the line that [minimizes the error]{style="text-decoration: underline;"} between observed and predicted values. If the relationship is far from linear, the regression coeficcients $\beta_0$ and $\beta_1$ will be heavily biased and the predictions will deviate significantly from the actual values or those you can gather in future experiments. Let's explore two basic tools to asses linearity in a regression model.

To ensure that linearity holds, we need to evaluate the relationship between predictors and the response variable. In the following section, we will explore two basic tools for assessing linearity in a regression model. 

The first and simplest way to assess linearity is to visualize the relationship between the predictor and response using a scatter plot, as demonstrated earlier in @fig-cell_data_scatter_EDA.

```{r}
#| label: fig-assumptions_linearity_scatter
#| fig-cap: Comparison between a linear and non linear trend
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false

library(ggplot2)

# We simulate a linear relatioship
set.seed(123)
x_linear <- seq(1, 100, by = 1)
y_linear <- 5 + 2 * x_linear + rnorm(100, mean = 0, sd = 10) # Linear relationship

# We simulate a non-linear relationship
x_nonlinear <- seq(1, 100, by = 1)
y_nonlinear <- 5 + 2 * x_nonlinear + 0.1 * x_nonlinear^2 + rnorm(100, mean = 0, sd = 10)

# We merge the data in a single dataframe
data_linear <- data.frame(x = x_linear, y = y_linear, type = "Linear Relationship")
data_nonlinear <- data.frame(x = x_nonlinear, y = y_nonlinear, type = "Non-linear Relationship")
data <- rbind(data_linear, data_nonlinear)

# We plot using ggplot
ggplot(data, aes(x = x, y = y)) +
  geom_point(alpha = 0.7) +
  facet_wrap(~type) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    title = "Assessing linear trends",
    x = "Predictor (x)",
    y = "Response (y)"
  ) +
  Plot_theme

```

In @fig-assumptions_linearity_scatter, we compare a linear relationship (left) and a nonlinear relationship (right). The graph on the left shows a linear trend where the best-fit line closely captures the observations across the range of the predictor. The graph on the right, however, illustrates a nonlinear relationship. The fitted line misses the lower values near the intercept ($0$) and over- or underestimates the upper values near $100$.

Does this mean you’ll get an error message if you attempt to fit a linear model to the second dataset? Not at all. The computer will faithfully do what you ask it to do—fit a line to the data. But that doesn’t mean the results will be meaningful. In this case, we can be confident that: The model’s estimates will be biased; and that both the intercept ($\beta_0$) and slope ($\beta_1$) will fail to reflect the true trend in the data.

This underscores a crucial point: a computer cannot replace your expertise or critical thinking. Recognizing whether a relationship is likely to be linear is your responsibility as a researcher and domain expert.

Keep in mind that many relationships in nature are not linear. Let’s take a simple, everyday example: weight and age. During childhood, there might be a roughly linear relationship between weight and age, as growth occurs at a steady pace. However, as you reach adolescence and adulthood, weight becomes influenced by many other factors (diet, activity level, metabolism, etc.), making age a much weaker predictor. Over time, weight does not continue to increase linearly—it might plateau, fluctuate, or even decrease depending on the individual and their circumstances.

::: callout-tip
It is essential considering biological and contextual knowledge when evaluating the assumptions of your model.
:::

Another effective way to assess linearity is through [residual plots]{style="color:green;"}, which we will explore in greater detail later in this book. Residual plots allow you to visualize the [difference]{style="text-decoration: underline;"} between [observed and predicted values]{style="color:green;"},highlighting patterns that may indicate a poor fit. If the relationship between variables is truly linear, the residuals should be [randomly distributed around zero]{style="color:green;"}. In contrast, the presence of
[strong or recognizable patterns]{style="color:red;"}, such as curves or systematic trends, is a clear sign of nonlinearity. 

```{r}
#| label: fig-assumptions_linearity_residuals
#| fig-cap: Comparison between a linear and non linear trend
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false

# We fit models
model_linear <- lm(y_linear ~ x_linear)
model_nonlinear <- lm(y_nonlinear ~ x_nonlinear)

# We calculate residuals
data_residuals <- data.frame(
  x = c(x_linear, x_nonlinear),
  residuals = c(residuals(model_linear), residuals(model_nonlinear)),
  type = rep(c("Linear Relationship", "Non-linear Relationship"), each = 100)
)

# We plot the residuals
ggplot(data_residuals, aes(x = x, y = residuals)) +
  geom_point(alpha = 0.7) +
  facet_wrap(~type) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residual plots for linearity check",
    x = "Predictor (x)",
    y = "Residuals"
  ) +
  Plot_theme

```

@fig-assumptions_linearity_residuals illustrates non-linearity, though it’s worth noting that real-world data often doesn’t display such obvious deviations. As we progress through this book, we’ll encounter examples and exercises that reflect the complexities of working with real data.

I bet you have seen that many books and tutorials recommend applying data transformations, such as logarithms or square roots, to linearize relationships that appear nonlinear. While these transformations can sometimes help, they often come at the cost of interpretability and biological meaning. On this topic, I align with the perspective of Pablo Inchausti [@inchausti2023], who refers to such transformations as "esoteric transformations." Inchausti argues that these are often unnecessary, particularly when we now have access to a wide range of advanced statistical modeling tools that can handle nonlinear relationships more directly and intuitively. Instead of forcing data into a linear framework, we can leverage these tools to model the data as it is, preserving its natural structure and meaning.

As we move forward, we’ll see how far we can go by avoiding unnecessary transformations and embracing models that respect the inherent complexity of our data. I believe this enhances our ability to draw meaningful and transparent conclusions from our research.

## Independence

The assumption of independence ensures that the observations in our dataset [are not systematically related to each other]{style="color:green;"}. In the context of linear regression, this means that the error terms ($\epsilon$) are not strongly correlated across observations. When this assumption is violated, we end up with [overlapping information]{style="color:red;"} from dependent observations, which reduces the efficiency of our model and undermines the reliability of its estimates.

Non-independence is a common issue in neuroscience. A typical example is [repeated measures]{style="color:green;"}, where multiple observations are taken from the same subject or animal. Imagine an experiment where a mouse is placed on a rotarod three times, and the time it takes to fall is recorded for each trial. These measurements are not independent, as they come from the same mouse and are likely influenced by shared factors (e.g., the mouse’s learning curve or fatigue). Treating these repeated measures as if they were independent observations in a regression model is problematic. While the model will still produce results—without error messages or warnings—it will lead to biased coefficients and less reliable estimates. Again, we have plenity of statistical tools to manage correlated observations, as we will see in CHAPTER XXX.

Let’s briefly simulate an example of non-independence for illustrative purposes:

First, we generate independent data, where the response variable 
$y$ depends linearly on time ($x$), with some added random, independent noise that is identically distributed. In this example, we calculate the `response variable` response_independent as a linear function of time with a slope of 0.3 and an intercept of 5. 

```{r}
#| label: tbl-assumptions-independent
#| tbl-cap: Simulated data for independent observations

library(gt)

# We generate independent time and response
set.seed(8807)
time_independent <- seq(1, 100, by = 1)
response_independent <- 5 + 0.3 * time_independent + rnorm(100, mean = 0, sd = 5)

# We combine them into a data frame
data_independent <- data.frame(
  time = time_independent,
  response = response_independent,
  type = "Independent"
)

gt(head(data_independent))
```

The result, shown in @tbl-assumptions-independent, demonstrates a random fluctuation in the response values, ranging from positive to negative numbers. 

Now, we introduce dependence into the data by incorporating autocorrelation. This means that each response value is partially influenced by the previous one. For this simulation, we create temporal dependence where adjacent observations are correlated, such that each observation depends not only on the linear relationship with time but also on 80% of the previous response value (0.8 * response_dependent[i - 1]).

```{r}
#| label: tbl-assumptions-dependent
#| tbl-cap: Simulated data for dependent observations


# Generate dependent data
response_dependent <- numeric(100)
response_dependent[1] <- 5 + rnorm(1, mean = 0, sd = 5) # First value
for (i in 2:100) {
  response_dependent[i] <- 5 + 0.3 * time_independent[i] +
    0.8 * response_dependent[i - 1] + rnorm(1, mean = 0, sd = 5)
}

# Combine into a data frame
data_dependent <- data.frame(
  time = time_independent,
  response = response_dependent,
  type = "Dependent"
)

gt(head(data_dependent))

```

The dependent data, shown in @tbl-assumptions-dependent, exhibits a clear increasing trend, driven by the systematic dependence introduced in our simulation.

By combining the two datasets, we can visualize the differences using `ggplot`, our trusted ally.

```{r}
#| label: fig-assumptions_independence
#| fig-cap: Comparison between dependent and independent observations
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false

library(ggplot2)

# We combine both datasets
data <- rbind(data_independent, data_dependent)

# We visualize data
ggplot(data, aes(x = time, y = response)) +
  geom_line() + # Line plot to show sequential patterns
  facet_wrap(~type) + # Separate plots for independent and dependent data
  labs(
    title = "Examining Independence of Observations",
    x = "Time",
    y = "Response"
  ) +
  Plot_theme


```

In @fig-assumptions_independence, the dependent observations resemble a rising stock in the financial market, while the independent observations look more like a less desirable, fluctuating stock.

In both cases, if we were to fit a model to investigate whether time affects the response, we would find an apparent relationship. However, if our observations are not independent and we fail to account for this, we risk greatly overestimating the effect of time on the response. The model will give us results, yes—but these results are likely to be spurious and misleading. Incredibly, this type of error is not uncommon. Many researchers, whether through oversight or lack of awareness, perform their analyses on dependent data as if the observations were independent. This includes cases where repeated measures from the same unit (e.g., the same animal, subject, or experimental setup) are treated as separate observations. 


## Homoscedasticity

I must admit, the assumption of [homoscedasticity]{style="color:green;"} is my favorite and also one of the [most violated]{style="color:green;"} in neuroscience and life sciences research. Honestly, overlooking homoscedasticity should make it hard to sleep peacefully at night! This assumption of linear regression specifies that the variance of the residuals (errors) must [remain constant]{style="text-decoration: underline;"} across all levels of the predictor(s). In simpler terms, no matter the value of the predictor variable(s), the spread of the residuals around the regression line should be similar.

Why does homoscedasticity matter? Becasue the calculation of [credible intervals and standard errors]{style="color:green;"} deppends on it. Without constant variance, these calculations—and the resulting inferences—become unreliable. And no, your linear model won’t check this for you. As far as your computer is concerned, if you ask it to fit a linear model, it assumes that you, the scientist, have verified that the data satisfies this property. For example, we saw in @sec-EDA_ggplot that data often violates this assumption, with variability increasing at higher values of predictors. Specifically, If the data does not have homoscedasticity, predictions from the model may be less reliable in certain ranges of the predictor(s). 

Apart from looking at the distribution of data points as shown in @fig-cell_data_rain_EDA, residual plots are once again a useful tool. For illustration, we will simulate two datasets—one with homoscedastic and another with heteroscedastic data. By fitting simple linear models to both datasets and plotting the residuals, we can clearly visualize the difference. 

```{r}
#| label: fig-assumptions_homoscedasticity
#| fig-cap: Comparison between homoscedastic and heterocedastic models.
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false


library(ggplot2)

# We simulate homoscedastic data
set.seed(123)
x_homoscedastic <- seq(1, 100, by = 1)
y_homoscedastic <- 5 + 0.5 * x_homoscedastic + rnorm(100, mean = 0, sd = 10)

# We simulate heteroscedastic data
x_heteroscedastic <- seq(1, 100, by = 1)
y_heteroscedastic <- 5 + 0.5 * x_heteroscedastic + rnorm(100, mean = 0, sd = x_heteroscedastic / 5)

# We combine data
data_homo <- data.frame(x = x_homoscedastic, y = y_homoscedastic, type = "Homoscedastic")
data_hetero <- data.frame(x = x_heteroscedastic, y = y_heteroscedastic, type = "Heteroscedastic")
data <- rbind(data_homo, data_hetero)

# We fit simple linear models
model_homo <- lm(y ~ x, data = data_homo)
model_hetero <- lm(y ~ x, data = data_hetero)

# Residual data for plotting
residual_data <- data.frame(
  fitted = c(fitted(model_homo), fitted(model_hetero)),
  residuals = c(residuals(model_homo), residuals(model_hetero)),
  type = rep(c("Homoscedastic", "Heteroscedastic"), each = 100)
)

# Residual plot
ggplot(residual_data, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.7) +
  facet_wrap(~type) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residual plots for visualizing homoscedasticity",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  Plot_theme

```

Similar to previous cases, @fig-assumptions_homoscedasticity demonstrates that residuals which are not evenly dispersed have something important to tell us about the assumptions and characteristics of our data. In this case, the figure on the left shows a [funnel-shaped pattern]{style="color:red;"}, where the spread of residuals increases as the fitted values (x-axis) augment. This is a common pattern observed when the variability of the data is narrow at certain levels of the predictor but increases significantly under different conditions or ranges.

Once again, instead of resorting to [esoteric transformations]{style="color:red;"} of the data, we can [model heteroscedasticity]{style="color:green;"}, specifying different [variance structures]{style="text-decoration: underline;"} within the model to account for changing variability (discussed in detail in CHAPTER XXX) or fitting [weighted regressions]{style="text-decoration: underline;"}, that allows us to assign different weights to observations based on their variability to build more robust models (covered in CHAPTER XXX). 

## Normality of residuals

In linear regression, the [normality assumption]{style="color:green;"} states that the residuals (errors) follow a normal distribution. This assumption is critical for many inferential tools associated with regression analysis, such as constructing credible intervals and prediction intervals. It is important to emphasize that this assumption applies specifically to the residuals—not to the predictors or the response variable.

::: callout-warning
The normality assumption applies to the residuals of the fitted model, not to the predictors or the response variable. However, it is common for non-normal responses to lead to non-normal residuals.
:::

The residuals ($e$ = $y$ -$\hat{y}$) are the differences between the observed values ($y$) and the predicted ones ($\hat{y}$) by the regression model. For a well-specified model, the residuals should be symmetrically distributed around zero, as we saw earlier. Note if they show systematic patterns or extreme skewness. If the residuals are not normal, you may be dealing with issues such as model misspecification, failure to capture non-linear trends or interactions; extreme values that distort the residual distribution; or heteroscedasticity, where the variability of the residuals varies across the range of predictors. 

It is worth noting that ordinary least squares (OLS) regression is generally considered robust to deviations from normality, particularly in large samples (REF). In such cases, the central limit theorem ensures that many inferential statistics derived from the regression model remain valid, even if the residuals are not perfectly normal. However, in smaller samples or when extreme deviations occur, violations of this assumption can compromise the accuracy of statistical inferences. Once again, I argue that we have sufficient statistical tools to build meaningful and robust models, rather than pushing a linear regression model to the extreme.

::: {#fig-fitmodels layout-ncol=1}
![](images/model_fit.webp){fig-align="center" width="60%"}

I see at least two approaches to data analysis and statistical modelling. One is the ritualistic option of using linear regression models for whatever kind of data you have, and trying to stretch the robustness and properties of these models as far as possible. Most of the time this will not be enough to build an informative model that fits your data and serves realistic scientific purposes. On the other hand, you can become an experienced modelling tailor, using the most advanced tools available in the XXI century to build clothes (models) that fit well and are adapted to the specific characteristics of the data you are working with. What will make you choose the first option? 
:::

To illustrate, let’s revisit the homoscedastic and heteroscedastic residuals from @fig-assumptions_homoscedasticity. By plotting their distributions as histograms, we can assess whether the residuals approximate a normal distribution or exhibit skewness or other deviations.

```{r}
#| label: fig-assumptions_normality_hist
#| fig-cap: Comparison between dependent and independent observations
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false

# We extract the residuals from the models
residual_data <- data.frame(
  residuals = c(residuals(model_homo), residuals(model_hetero)),
  type = rep(c("Homoscedastic residuals", "Heterocedastic residuals"), each = 100)
)

# We generate histograms
ggplot(residual_data, aes(x = residuals)) +
  geom_histogram(bins = 20, alpha = 0.7, fill = "blue") +
  facet_wrap(~type, scales = "free") +
  labs(
    title = "Histogram of Residuals",
    x = "Residuals",
    y = "Frequency"
  ) +
  Plot_theme

```
We observe in fig-assumptions_normality_hist that normal residuals resemble the familiar bell shape around zero, a hallmark of normal distributions. I truly believe this simple approach is more informative that certain tests people uses as a requirement of journals and reviwers without even knowing what the test is testing.  

Another effective tool for assessing the normality of residuals is the Q-Q plot (quantile-quantile plot), which compares the observed quantiles of residuals to the theoretical quantiles of a normal distribution. For residuals that follow a normal distribution, the points will align closely with a diagonal line.

```{r}
#| label: fig-assumptions_normality_qq
#| fig-cap: Comparison between dependent and independent observations
#| fig-width: 10
#| fig-height: 6
#| warning: false
#| message: false

# Generate Q-Q plot for residuals
ggplot(residual_data, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  facet_wrap(~type) +
  labs(
    title = "Q-Q Plot of Residuals",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  Plot_theme

```

As shown in @fig-assumptions_normality_qq, Q-Q plots often highlight deviations from normality more clearly than histograms. In the case of heteroscedastic data, we see that the residuals deviate substantially from the diagonal line, particularly at the lower (left) and upper (right) extremes. This pattern is characteristic of data that violates the homoscedasticity assumption, where the variability of residuals changes across levels of the predictor(s). In the book, we will generate multiple Q-Q plots to diagnose our models. 
