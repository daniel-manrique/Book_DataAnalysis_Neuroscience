{
  "hash": "4d47b568c87ee56bdc5aae0e0394019e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nbibliography: references.bib\n---\n\n\n\n\n\n# What is linear regression and what are its assumptions? {#sec-regression-assumptions}\n\n\n\n\n\n\n\n\n\n\n\n\n## What is Linear Regression?\n\nLet's introduce linear regression as a statistical approach to [modelling and analysing relationships between variables]{style=\"color:green;\"}.\nIn one of his lectures, Richard McElreath summarises the essence of linear regression as:\n\n::: {style=\"text-align: center\"}\n***what's the value of knowing A if I know B?***\n:::\n\nWe can also say that it focuses on understanding how changes in one or more independent variables (also called predictors or explanatory variables) are related to changes in a dependent variable (better known as the response).\nIn this way, linear regression provides a basis for [describing and predicting responses]{style=\"color:green;\"} from data sets.\n\nAt its core, linear regression is based on the premise that the relationship between variables can be [approximated]{style=\"text-decoration: underline;\"} by a **straight line** (or a hyperplane when dealing with multiple variables).\nThis line allows researchers and data analysts to:\n\n-   **Find relationships and patterns** between variables.\n\n-   **Make predictions** for unknown values.\n\n-   **Test scientific hypotheses** quantifying the degree of association between responses and predictors.\n\nIn simple terms, I like to say that regression allows researchers to answer questions about the influence of particular predictors on the response.\n\n## The Linear Regression Equation\n\nLinear regression, in its simplest version, relies on a consise mathematical model that represent the relationship between the response and its predictors.\nThe [most simple linear regression model]{style=\"text-decoration: underline;\"} takes the following form:\n\n$$\ny = \\beta_0 + \\beta_1x + \\epsilon\n$$ {#eq-linear-regression-simple}\n\nLet's define the different constituents of the equation:\n\n-   $y$ represent the [response]{style=\"color:green;\"}, the outcome we want to explain or predicts using our predictors.\n    In neuroscience, this can take many masks, like the number of cells, the affected area, the proportion of death cells, etc.\n\n-   We represent the [predictor(s)]{style=\"color:green;\"} with $x$.\n    These are the factors we belive influence the outcome $y$.\n    For instance, days after injury, treatment, drug dosis, experimental group, etc.\n\nBoth, the outcome and predictors are in our data frames.\nWe now then in advance and we are able to explore them using EDA.\nWhat the magic of linear regression and to our understanding are the following terms:\n\n-   $\\beta_0$ is called the [intercept]{style=\"color:green;\"}.\n    This coefficient tells us what it the value of our response $y$ when the predictor $x$ is at it's base level, if it a factor or category, or at $0$ when it is a numeric variable.\n    For instance, if we measure cognitive performance ($y$), as a function of reaction time ($x$), $\\beta_0$ reflects the predicted cognitive score for individuals with a (theorical) reaction time of zero.\n\n-   $\\beta_1$ is know as the [slope]{style=\"color:green;\"}.\n    This coefficient indicates how much the response $y$ changes when we move to the non-base value (when the variable is cataegorical) or how much or response change for a one-unit increase in $x$ (when the variable is numerical).\n    Taking the same example as above, $\\beta_1$ tells us how much the predicted cognitive score increases per second (reaction time), assuming that we measure the reaction time in seconds.\n\nIn the most simple cases, these two coefficients are the most important elements we need to know to make strong scientific inferences, and we will navigate them further as we advance in this book.\nSurprisingly, you can be confident that the intercept and slope are more informative than the lowest p-value you have ever seen in your life, or the graph adorned with most asterisks, which are not at all useful in statistical inference.\n\n-   Fianlly, $\\epsilon$ is know as the error term, which account for the variability in the response $y$ that is not explained by the predictors $x$.\n\nAt the end, what a regression does, is to do its best to estimate $\\beta_0$ and $\\beta_1$, based on the observed data (our data frames and tables), minimizing the discrepancy between the variables we see in real life (in our table) $y$ and the values $Y$ that our model predicts.\n\nWhat happen when we have multiple predictors, like reaction time, and treatment.\nWell, the @eq-linear-regression-simple generalizes to:\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_kx_k + \\epsilon\n$$ {#eq-linear-regression-multiple} In this case, $x_1$, $x_2$ + $\\dots$ are the additional predictors (indepemndent variables).\nThis means that a linear regression allows us to investigate how multiple factors or conditions simultaneously influence the response variable.\nEven, we can control for the effect of other predictors.\nI am still impressed at how little use we make of these approaches in neuroscience, and at how little understanding we neuroscientists have of them.\n\n## Assumptions of Linear Regression\n\n### Linearity\n\nThe assumption of linearity is perhaps the most foundational aspect of linear regression, therefore its name.\nThis assumption stipulates that the relationship between the predictor(s) and the response variable is linear.\nIn other words, that changes in the predictor lead to proportional changes in the response.\nMathematically, we define this relationship as stated in @eq-linear-regression-simple.\nIf this assumption is violated, the model may fail to capture the true nature and magnitude of the relationship and lead us to biased estimates and poor predictive performance.\nThe main issue here is that when you execute a linera regression you will not find an error message if this assumption is violated.\nThe cumputer does what you ask it to do and the code was no way of reasoning about this assumption.\nAs a scientists, you are the one defining if your relatiship between variables is likely to be linear by doing EDA.\nThat's your job, not the computers job.\n\nSo, linearity in a linear regression matters becuase we aim to estimate the best-fit line, one that minimizes the error between observed values and predicted values.\nIf the relationship is far for linear linear, the regression coeficcients $\\beta_0$ and $\\beta_1$ will be heavily biased and the predictions will deviate significantly from the actual values or those you can gather in future experiments.\nYou need to know that getting an outcome for a regression code it is not sufficient.\nYou need to be sure that your procedure leads to reliable inferences.\nLet's explore two basic tools to asses linearity in a regression model.\n\nFirst, we can visualize the predictor and response using a scatter plot, as we have seen previusly in @sec-EDA_ggplot.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# We simulate a linear relatioship\nset.seed(123)\nx_linear <- seq(1, 100, by = 1)\ny_linear <- 5 + 2 * x_linear + rnorm(100, mean = 0, sd = 10) # Linear relationship\n\n# We simulate a non-linear relationship\nx_nonlinear <- seq(1, 100, by = 1)\ny_nonlinear <- 5 + 2 * x_nonlinear + 0.1 * x_nonlinear^2 + rnorm(100, mean = 0, sd = 10)\n\n# We merge the data in a single dataframe\ndata_linear <- data.frame(x = x_linear, y = y_linear, type = \"Linear Relationship\")\ndata_nonlinear <- data.frame(x = x_nonlinear, y = y_nonlinear, type = \"Non-linear Relationship\")\ndata <- rbind(data_linear, data_nonlinear)\n\n# We plot using ggplot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.7) +\n  facet_wrap(~type) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Assessing linear trends\",\n    x = \"Predictor (x)\",\n    y = \"Response (y)\"\n  ) +\n  Plot_theme\n```\n\n::: {.cell-output-display}\n![Comparison between a linear and non linear trend](regression-assumptions_files/figure-html/fig-assumtions_linearity_scatter-1.png){#fig-assumtions_linearity_scatter width=960}\n:::\n:::\n\n\n\n\n\n@fig-assumtions_linearity_scatter shows a linear and non-linear relationship.\nWhile the graph of the left captures closely our observations, the one on the right misses the lower values close to the intercept ($0$), and the upper values, close to $100$.\nDoes it mean we will get an error message of we fit a linear model two our second dataset?\nNot at all.\nThe computer will do what you askit to do: to fir a line to that data.\nHowever, we can be certain that our estimations will be heavily biased, and that both, our intercep and slope will not reflect the true trend we expect to see in the data.\nA computer can not replace your brain here, neither your expertice in a subject.\n\nWhat aspects of nature are not linear?\nI give you en easy one, for daily live.\nWeight and age.\nWee know that this relation is not linear because we do not increse weight in a linear fashion with time.\nThere may be a slight linear relationship between 0 and teen age.\nThereafter, your weight will depend of many other factors, being the age one of the less important.\nYou will not continue increasing weight linearly as you did when you were growing as a child.\n\nAnother way of assesing linearity are residual plots, which we will explore some pages later in the book.\nResidual plots allow you to visdualize the difference between observed and predicted values.\nIt the relationshoit is linear, residuals should be randomly distributed around zero.\nWhen you see strang or catchy patterns, including systematic curves or trends, you have strong signs of nonlinearity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We fit models\nmodel_linear <- lm(y_linear ~ x_linear)\nmodel_nonlinear <- lm(y_nonlinear ~ x_nonlinear)\n\n# We calculate residuals\ndata_residuals <- data.frame(\n  x = c(x_linear, x_nonlinear),\n  residuals = c(residuals(model_linear), residuals(model_nonlinear)),\n  type = rep(c(\"Linear Relationship\", \"Non-linear Relationship\"), each = 100)\n)\n\n# We plot the residuals\nggplot(data_residuals, aes(x = x, y = residuals)) +\n  geom_point(alpha = 0.7) +\n  facet_wrap(~type) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Residual plots for linearity check\",\n    x = \"Predictor (x)\",\n    y = \"Residuals\"\n  ) +\n  Plot_theme\n```\n\n::: {.cell-output-display}\n![Comparison between a linear and non linear trend](regression-assumptions_files/figure-html/fig-assumtions_linearity_residuals-1.png){#fig-assumtions_linearity_residuals width=960}\n:::\n:::\n\n\n\n\n\n@fig-assumtions_linearity_residuals illustrates the point, although things with real data, in real life are not as obvious, as we will see in further exercises in this book.\nIn many books and tutorials, you will have approximations that make transformations to the data, including log and square root to make the relationship non-linear.\nConcerning this point, I have a perspective similar to that of Pablo Inchausti [@inchausti2023], who refers to it as \"esoteric transformations\", unnecessary when we have a wide range of statistical modeling tools at our disposal.\nLet's see how far we can go embracing this idea.\n",
    "supporting": [
      "regression-assumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}